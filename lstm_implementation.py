# -*- coding: utf-8 -*-
"""LSTM IMPLEMENTATION

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oZYuyIi87Y6tP-78c-1HkaPY1g9ffJIB
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load your dataset
df = pd.read_csv('/content/POWER GENERATION DS.csv')

# Filter data for Punjab station
punjab_data = df[df['Power Station'] == 'Punjab']

# Keep the 'Date' column separately for later use
dates = punjab_data['Dates']

# Drop unnecessary columns
punjab_data = punjab_data.drop(['Power Station', 'Dates'], axis=1)

# Replace zeros with NaN, then replace NaNs with column mean
punjab_data = punjab_data.replace(0, np.nan)
punjab_data = punjab_data.fillna(punjab_data.mean())

# Concatenate the 'Date' column back to the cleaned data
punjab_data['Dates'] = dates


punjab_data['Dates'] = pd.to_datetime(punjab_data['Dates']).dt.strftime('%Y-%m-%d')
punjab_data['Dates'] = pd.to_datetime(punjab_data['Dates'])  # Reconvert to datetime
punjab_data.set_index('Dates', inplace=True)

punjab_data

from sklearn.preprocessing import MinMaxScaler
import numpy as np


# Define the target variable and features
target_column = 'Actual(MU)'
feature_columns = [col for col in punjab_data.columns if col != target_column and col != 'Dates']

# Initialize separate scalers for features and the target
scaler_features = MinMaxScaler(feature_range=(0, 1))
scaler_target = MinMaxScaler(feature_range=(0, 1))

# Scale features
features_scaled = scaler_features.fit_transform(punjab_data[feature_columns])

# Scale target
target_scaled = scaler_target.fit_transform(punjab_data[[target_column]])

# Combine scaled features and target into a single dataset
scaled_data = np.hstack((features_scaled, target_scaled))

import pandas as pd

# Convert features_scaled back to a DataFrame for info()
features_scaled_df = pd.DataFrame(features_scaled, columns=feature_columns, index=punjab_data.index)

# Now you can call info() on the DataFrame
features_scaled_df.info()

# Create a dataset function for LSTM with multiple features
def create_dataset_multi_feature(data, time_step=1):
    X, y = [], []
    num_features = data.shape[1] - 1  # Last column is the target
    for i in range(len(data) - time_step):
        X.append(data[i:(i + time_step), :-1])  # Select all columns except target
        y.append(data[i + time_step, -1])  # Select only the target column
    return np.array(X), np.array(y)

# Create the dataset
time_step = 30
X, y = create_dataset_multi_feature(scaled_data, time_step)

# Reshape X for LSTM input
X = X.reshape(X.shape[0], X.shape[1], X.shape[2])

# Split data
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(time_step, X.shape[2])))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))
model.add(Dense(units=1))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), batch_size=32)

from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Predict on the test set using the trained model
y_pred_scaled = model.predict(X_test)

# Inverse transform the predictions and the actual values
y_pred = scaler_target.inverse_transform(y_pred_scaled.reshape(-1, 1))
y_test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1))

# Calculate evaluation metrics
mse = mean_squared_error(y_test_actual, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_actual, y_pred)

# Print the evaluation metrics
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'RÂ² Score: {r2}')

# Plot the actual vs predicted values for the test set
plt.figure(figsize=(12, 6))
plt.plot(y_test_actual, color='blue', label='Actual Values')
plt.plot(y_pred, color='red', label='Predicted Values')
plt.title('Actual vs Predicted Power Generation (Actual MU) for the Test Set')
plt.xlabel('Time')
plt.ylabel('Power Generation (Actual MU)')
plt.legend()
plt.show()

forecast_steps = 30
forecast_values = []

# Get the last 'time_step' sequence from the test set
last_data = X_test[-1].reshape(1, time_step, X.shape[2])

for _ in range(forecast_steps):
    # Predict next value
    next_prediction_scaled = model.predict(last_data)

    # Inverse transform only the target using `scaler_target`
    next_prediction = scaler_target.inverse_transform(next_prediction_scaled.reshape(-1, 1))
    forecast_values.append(next_prediction[0, 0])

    # Prepare next input sequence by updating the last_data
    # Take last_data and update with the predicted value in the feature columns
    new_input = last_data[:, 1:, :]  # Shift the window
    # We assume the new forecasted value is appended as a new target
    new_feature_vector = np.hstack([last_data[:, -1, :-1], next_prediction_scaled]).reshape(1, 1, -1)
    last_data = np.concatenate([new_input, new_feature_vector], axis=1)

# Print the forecasted values
print(f"Forecasted Power Generation for the next 30 days: {forecast_values}")

punjab_data

import matplotlib.pyplot as plt
import pandas as pd

# Inverse transform the actual target data for the last 30 days using `scaler_target`
actual_data_last_30_days = scaler_target.inverse_transform(scaled_data[-30:, -1].reshape(-1, 1))

# Create the date range for the forecast (next 30 days)
forecast_dates = pd.date_range(punjab_data.index[-1] + pd.Timedelta(days=1), periods=30, freq='D')

# Plotting
plt.figure(figsize=(12, 6))
plt.plot(punjab_data.index[-30:], actual_data_last_30_days, label='Actual Data (Last 30 days)', marker='o')
plt.plot(forecast_dates, forecast_values, label='Forecast (Next 30 days)', marker='x', color='red')

plt.xlabel('Date')
plt.ylabel('Power Generation (MU)')
plt.title('Forecast for the Next 30 Days - Punjab Power Station')
plt.xticks(rotation=45)
plt.legend()
plt.grid()
plt.show()

pip install pycausalimpact

import pandas as pd
import numpy as np
from causalimpact import CausalImpact
import matplotlib.pyplot as plt

# Define the intervention date (your intervention date: '2021-03-24')
intervention_date = '2021-03-24'

# Create a new DataFrame with the required columns for Causal Impact
ci_data = pd.DataFrame({
    'Date': punjab_data.index,
    'Actual': punjab_data['Actual(MU)'],
})

# Ensure that the data is sorted by the datetime index
ci_data = ci_data.sort_index()

# Check the index to ensure it's a valid DatetimeIndex
print(ci_data.index)

# Set Date as the index
ci_data.set_index('Date', inplace=True)

# Check for duplicate datetime values
print(ci_data.index.duplicated().sum())  # Number of duplicates
print(ci_data[ci_data.index.duplicated()])  # Display duplicate rows

ci_data = ci_data[~ci_data.index.duplicated(keep='first')]  # Keep the first occurrence and remove duplicates


# Define the pre and post-intervention periods
pre_period = [ci_data.index.min(), pd.to_datetime(intervention_date) - pd.Timedelta(days=1)]  # End one day before intervention
post_period = [pd.to_datetime(intervention_date), ci_data.index.max()]  # Start on the intervention date

# Ensure there is a clear split by converting to string type
pre_period = [str(date) for date in pre_period]
post_period = [str(date) for date in post_period]

# Running the Causal Impact analysis
impact = CausalImpact(ci_data['Actual'], pre_period, post_period)

# Print summary of the analysis
print(impact.summary())
print(impact.summary(output='report'))

# Plot the results
impact.plot(figsize=(12, 8))
plt.show()

pip install shap

import shap
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# Ensure you have your LSTM model ready, trained, and `X_test` defined before this step.

# Step 1: Prepare `X_test_shap` for SHAP analysis
# Flatten `X_test` from 3D (samples, time_steps, features) to 2D
n_samples = X_test.shape[0]             # Number of samples in X_test
time_steps = X_test.shape[1]            # Number of time steps (e.g., 30)
n_features = X_test.shape[2]            # Number of features (e.g., 8)

# Flatten the 3D X_test to 2D
X_test_shap = X_test.reshape(n_samples, time_steps * n_features)

# Verify the shape of X_test_shap
print("Shape of X_test_shap:", X_test_shap.shape)  # Expected shape: (n_samples, time_steps * n_features)

# Step 2: Define the wrapper function for LSTM predictions
def lstm_predict_wrapper(X_flattened):
    """
    Wrapper function for the LSTM model to reshape input data and make predictions.
    """
    # Reshape from (n_samples, time_steps * n_features) to (n_samples, time_steps, n_features)
    reshaped_input = X_flattened.reshape(-1, time_steps, n_features)

    # Predict using the trained LSTM model
    predictions_scaled = model.predict(reshaped_input)

    # Inverse transform the predictions to get actual values
    predictions_actual = scaler_target.inverse_transform(predictions_scaled.reshape(-1, 1))
    return predictions_actual.flatten()

# Step 3: Initialize KernelExplainer with the wrapper function
explainer = shap.KernelExplainer(lstm_predict_wrapper, X_test_shap[:50], link="identity")

# Step 4: Compute SHAP values for the first 100 samples
print("Computing SHAP values. This may take a while...")
shap_values = explainer.shap_values(X_test_shap[:50])

# Step 5: Reshape SHAP values from (100, 240) to (100, time_steps, n_features)
shap_values_reshaped = np.array(shap_values).reshape(50, time_steps, n_features)

# Step 6: Aggregate SHAP values across time steps (average impact for each feature)
shap_values_avg = np.mean(shap_values_reshaped, axis=1)

# Step 7: Generate SHAP summary plot for all features
feature_columns = [col for col in punjab_data.columns if col != 'Actual(MU)' and col != 'Dates']
print("Generating SHAP summary plot...")
shap.summary_plot(shap_values_avg, X_test_shap[:50, :n_features], feature_names=feature_columns)
shap.force_plot(explainer.expected_value, shap_values_avg[0], X_test_shap[:1, :n_features], feature_names=feature_columns)

